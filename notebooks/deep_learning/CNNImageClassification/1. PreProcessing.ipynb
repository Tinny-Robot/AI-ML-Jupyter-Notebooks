{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a9fb5f2",
   "metadata": {},
   "source": [
    "## Welcome to the AI-ML-Jupyter-Notebooks repository! \n",
    "### This guide will help you navigate and learn CNN Image Classification using TensorFlow and OpenCV.\n",
    "\n",
    "----\n",
    "\n",
    "#### Make sure to install necessary dependencies by running this command :\n",
    "pip install -r requirements.txt\n",
    "\n",
    "---\n",
    "\n",
    "#### TensorFlow is Required to run this codebase.\n",
    "Install using the Official Guige [HERE](https://www.tensorflow.org/install/pip).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9736387-67a9-454a-8a07-402e0f20b9ac",
   "metadata": {},
   "source": [
    "### Understanding PreProcessing\n",
    "- Load Dataset from 'DATA_DIR' Directory\n",
    "- Creates DataFrame containing Full Paths of Images and their Class Labels\n",
    "- (Change as per Requirement) Rescale Images to Computationally Efficient Resolution\n",
    "- (Optional but Recommended) Extracts Largest Object from Image using 'image_processing' Function\n",
    "    - Leverages Parallel Processing for Faster Results\n",
    "- Compares Original and Rescaled+Processed Image SIde-By-Side to make necessary changes\n",
    "- Converts Processed Images to NumPy Array and Exports as Pickle File\n",
    "    - Verifies If Exported Pickle File is Appropriate through 10 Random Samples\n",
    "- (Optional) Merge Certain Class Lables Together\n",
    "- Split Data for Training, Testing, Validation with Stratify to ensure data balancing\n",
    "    - Verify if Split is Appropriate through 2 random samples\n",
    "- (Optional) Perform Random Oversampling on Data to reduce Biasness\n",
    "    - Verify if Oversampling is Appropriate through 2 random samples\n",
    "- Perform One-Hot-Encoding of Class Labels\n",
    "- Training, Testing, Validation Data and One-Hot-Encoding is Exported as Pickle Files\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4553514b-e3b6-4be4-9c88-533531557a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Working Directories\n",
    "'''\n",
    "import os\n",
    "\n",
    "# Directory of Original Dataset\n",
    "DATA_DIR = '../Dataset'\n",
    "\n",
    "# Directory where Pickle Files will be Stored (Folder Will be Created by Code)\n",
    "PICKLE_DIR = '../PickleFiles/'\n",
    "os.makedirs(os.path.dirname(PICKLE_DIR), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277ef5b7-6a3a-4759-8912-c9d9282f7f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Importing Necessary Libraries and Packages\n",
    "'''\n",
    "\n",
    "# Helpers\n",
    "import random\n",
    "import pickle\n",
    "import concurrent.futures\n",
    "from HelperFunctions import images_on_side, image_processing\n",
    "\n",
    "# Data Handling and Visualization\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Image Processing\n",
    "import cv2\n",
    "from skimage.io import imread as sk_imread\n",
    "\n",
    "# Model Pipelining \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48da2009-4b18-4ffe-b0de-89f929b7142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Creating a DataFrame containing \n",
    "    - Paths of All Source Image Files and their respective Class Label\n",
    "'''\n",
    "\n",
    "def get_file_paths(main_dir):\n",
    "    file_paths = []\n",
    "    damage_class = []\n",
    "    df = pd.DataFrame()\n",
    "    for root, dir, files in os.walk(main_dir):\n",
    "        if root == main_dir:\n",
    "            continue\n",
    "        class_damage = os.path.basename(root)\n",
    "        lst = [os.path.join(root, filename) for filename in files]\n",
    "        file_paths.extend(lst)\n",
    "        damage_class.extend([class_damage] * len(lst))\n",
    "        del(lst) # Clear RAM\n",
    "        del(class_damage) # Clear RAM\n",
    "    df['File Path'] = file_paths\n",
    "    df['Class Label'] = damage_class\n",
    "    del(file_paths) # Clear RAM\n",
    "    del(damage_class) # Clear RAM\n",
    "    return df\n",
    "\n",
    "# The DataFrame\n",
    "df = get_file_paths(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447261de-79cb-4f34-a024-a4cf8765e53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Details About Created DataFrame\n",
    "\n",
    "print('Shape of Created DataFrame : ', df.shape)\n",
    "print('Total Number of Sample Images : ',len(df))\n",
    "print('Total Number of Class Label : ',len(set(df['Class Label'])))\n",
    "print('\\nNumber of Samples for Each Class Label :\\n',df.groupby('Class Label')['File Path'].count().to_string()[12:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f143a47-dcf4-45a8-83d3-69b828690962",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Verify if there is any Redundancy in DataFrame \n",
    "    - Ideally, There should be No Redundancy\n",
    "Verify if all File Paths exist in Mentioned Directory\n",
    "    - Ideally, All Paths should Exist\n",
    "Export Entire DataFrame as a Pickle File\n",
    "    - Only If there is No Redundancy and All Paths Exist\n",
    "'''\n",
    "\n",
    "# Verify : Duplicate Values / Redundancy\n",
    "redundancy = True\n",
    "duplicates = df[df['File Path'].duplicated(keep=False)]\n",
    "if len(duplicates)>0:\n",
    "    print(\"Redundancy Identified As :\")\n",
    "    print(duplicates['File Path'])\n",
    "else:\n",
    "    print('There is No Redundancy.')\n",
    "    redundancy = False\n",
    "del(duplicates) # Clear RAM\n",
    "\n",
    "# Verify : All Paths Exist\n",
    "validity = False\n",
    "paths = list(df['File Path'])\n",
    "nonexistent_paths = [path for path in paths if not os.path.exists(path)]\n",
    "if len(nonexistent_paths)>0:\n",
    "    print(\"Non-Existent Paths Identified As :\")\n",
    "    print(nonexistent_paths)\n",
    "else:\n",
    "    print('All Paths Exist.')\n",
    "    validity = True\n",
    "del(paths)  # Clear RAM\n",
    "del(nonexistent_paths)  # Clear RAM\n",
    "\n",
    "# Export DataFrame as a Pickle File if all conditions are met\n",
    "if redundancy == False and validity == True:\n",
    "    df.to_pickle(PICKLE_DIR+\"FilePathsAndClassLabels.pkl\")\n",
    "    print('DataFrame Successfully Exported as Pickle File.')\n",
    "else:\n",
    "    print('Cannot Export DataFrame, Check for Redundancy and Validity of File Paths')\n",
    "del(redundancy) # Clear RAM\n",
    "del(validity) # Clear RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752761ee-1990-40cb-828c-40472de7aa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    ** Verification Only **\n",
    "Getting Random Sample (Image) from Dataset\n",
    "Checking Dimentions of Obtained Random Sample\n",
    "Extracting Largest Object from Image (Optional but Recommended)\n",
    "Resize Random Sample to 600X400 (Optional but Recommended)\n",
    "Visualizing Original and Resize Sample Side-by-Side\n",
    "    - Ideally, Images Displayed Side-By-Side should be Similar\n",
    "'''\n",
    "\n",
    "# Obtaining and Checking Dimentions of a Random Image from Dataset\n",
    "path = random.choice(df['File Path'])\n",
    "sample_img = sk_imread(path)\n",
    "print('Original Dimentions of Random Sample : ',sample_img.shape)\n",
    "\n",
    "# Extracting Largest Object from Image (Optional)\n",
    "processed_img = image_processing(path)\n",
    "\n",
    "# Rescaling and Verifying \n",
    "IMG_HEIGHT = 400\n",
    "IMG_WIDTH = 600\n",
    "IMG_CHANNELS = 3 # Assuming RGB Image\n",
    "processed_img = cv2.resize(processed_img, (IMG_WIDTH, IMG_HEIGHT))\n",
    "print('Dimentions of ProcessedRandom Sample : ',processed_img.shape)\n",
    "\n",
    "# Visualizing Original and Processed Random Image\n",
    "images_on_side(sample_img,'Original Random Sample',processed_img,'Processed Random Sample')\n",
    "\n",
    "\n",
    "del(path) # Clear RAM\n",
    "del(sample_img) # Clear RAM\n",
    "del(processed_img) # Clear RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f05199-c4e4-4a99-ae16-2e6be388df1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "*Using Parallel Processing for Faster Results*\n",
    "Extracting Largest Object from Image and Change Background to Black\n",
    "Resize All Processed Images to 600X400 (Optional but Recommended)\n",
    "Convert Images to Numpy Arrays\n",
    "    - Later referred to as ImageArray\n",
    "Export Image Arrays as Pickle File \n",
    "    - Exporting is done after every 'export_after' number of Images\n",
    "'''\n",
    "\n",
    "export_after = 1000\n",
    "\n",
    "def process_images_chunk(chunk, start_index):\n",
    "    chunk_result = np.zeros((len(chunk), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), np.uint8)\n",
    "    for i, pth in enumerate(chunk):\n",
    "        img = image_processing(pth)\n",
    "        chunk_result[i] = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
    "    return start_index, chunk_result\n",
    "\n",
    "chunks = [df['File Path'][i:i+export_after].tolist() for i in range(0, len(df['File Path']), export_after)]\n",
    "\n",
    "X = np.zeros((len(df), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), np.uint8)\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    futures = {executor.submit(process_images_chunk, chunk, i*export_after): i for i, chunk in enumerate(chunks)}\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        start_index, result = future.result()\n",
    "        end_index = start_index + len(result)\n",
    "        X[start_index:end_index] = result\n",
    "        print(f'Processed chunk starting at index: {start_index}')\n",
    "\n",
    "# Save the merged result\n",
    "with open(PICKLE_DIR + 'ImageArrays.pkl', 'wb') as f:\n",
    "    pickle.dump(X, f)\n",
    "print('Image Array Export Completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f955d706-00c1-491c-8945-d0b6f1cbff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    ** Verification Only **\n",
    "Index 'i' of DataFrame should reflect the DamageClass for Image at Index 'i' in ImageArray \n",
    "    - This is done as a Verification Step before using Data for Model Training/Testing.\n",
    "    - Verification Done using 10 Random Samples.\n",
    "    - NOTE : Ideally, All Images should match the Labels\n",
    "'''\n",
    "\n",
    "# Select 10 Random Images for Verification\n",
    "inds = list(np.random.randint(0, len(df), 10))\n",
    "\n",
    "# Show Images from Image Array and Class Label from DataFrame\n",
    "for i in range(0,10,2):\n",
    "    images_on_side(X[inds[i]],df['Class Label'].iloc[inds[i]],X[inds[i+1]],df['Class Label'].iloc[inds[i+1]])\n",
    "\n",
    "del(inds) # Clear RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760ab34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    ** Optional Step **\n",
    "Merging Class Labeles\n",
    "    - By Default, Merging of Damaged Classes is Disabled\n",
    "'''\n",
    "\n",
    "# Set 'merge' to True if Merging of Damaged Classes is Needed\n",
    "merge = False\n",
    "\n",
    "if merge == True :\n",
    "    # List of Class Labeles to Merge\n",
    "    to_merge = ['ClassLabel1', 'ClassLabel2', 'ClassLabel3', 'ClassLabel4']\n",
    "    # Class Label to Merge Into\n",
    "    merge_into = 'CombinedClassLabel'\n",
    "    # Merging and Verification\n",
    "    df['Class Label'].replace(to_merge, merge_into, inplace=True)\n",
    "    print('Merging Of Class Label is Successful.')\n",
    "    print('Number of Samples for Each Class Label : \\n',df.groupby('Class Label')['File Path'].count().to_string()[12:])\n",
    "    del(to_merge) # Clear RAM\n",
    "    del(merge_into) # Clear RAM\n",
    "else:\n",
    "    print('Merging Of Class Label is Disabled. \\n')\n",
    "    print('Number of Samples for Each Class Label : \\n',df.groupby('Class Label')['File Path'].count().to_string()[12:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae6b83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Split the Data for Training, Testing, Validation\n",
    "  - SEED ensures that Generated Splits are Reproducible.\n",
    "  - Both DataFrame and ImageArrays are Split for Training and Testing\n",
    "  -'stratify' ensures that Proportion of Items in Splits is Same as that in the Parameter\n",
    "'''\n",
    "\n",
    "# Splitting Data\n",
    "SEED = 50\n",
    "test_size = 0.15\n",
    "val_size = 0.15\n",
    "temp_size = test_size / (test_size + val_size) # proportion of test and validation set\n",
    "\n",
    "# Split DamageClassLabels and ImgArray into Training and Temporary (will be split into Testing and Validation).\n",
    "ClassLabels_train, ClassLabels_temp, ImgArray_train, ImgArray_temp = train_test_split(df, X, stratify=df['Class Label'], shuffle=True, test_size=test_size+val_size, random_state=SEED)\n",
    "del(df) # Clear RAM\n",
    "del(X) # Clear RAM\n",
    "\n",
    "# Split temporary datasets into Testing and Validation.\n",
    "ClassLabels_validation, ClassLabels_test, ImgArray_validation, ImgArray_test = train_test_split(ClassLabels_temp, ImgArray_temp, stratify=ClassLabels_temp['Class Label'], shuffle=True, test_size=temp_size, random_state=SEED)\n",
    "del(ClassLabels_temp) # Clear RAM\n",
    "del(ImgArray_temp) # Clear RAM\n",
    "\n",
    "del(SEED) # Clear RAM\n",
    "del(test_size) # Clear RAM\n",
    "del(val_size) # Clear RAM\n",
    "del(temp_size) # Clear RAM\n",
    "\n",
    "# Getting Overview of Split Data\n",
    "print('Item:','ClassLabels_train','        Shape:',ClassLabels_train.shape)\n",
    "print('Item:','ClassLabels_test','         Shape:',ClassLabels_test.shape)\n",
    "print('Item:','ClassLabels_validation','   Shape:',ClassLabels_validation.shape)\n",
    "print('Item:','ImgArray_train','           Shape:',ImgArray_train.shape)\n",
    "print('Item:','ImgArray_test','            Shape:',ImgArray_test.shape)\n",
    "print('Item:','ImgArray_validation','      Shape:',ImgArray_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f3c75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    ** Verification Only **\n",
    "Verify that Data Split is Successful.\n",
    "  - Index 'i' of ImgArray_train should have exactly same image as Index 'i' of ClassLabels_train\n",
    "  - Index 'i' of ImgArray_test should have exactly same image as Index 'i' of ClassLabels_test\n",
    "  - NOTE : Ideally, Images on Left and Right should be Exactly Same\n",
    "  \n",
    "'''\n",
    "\n",
    "# Index 'i' of ImgArray_train should have exactly same image as Index 'i' of df_train\n",
    "ind = random.randint(0, len(ClassLabels_train))\n",
    "images_on_side(ImgArray_train[ind],'Sample from ImgArray_train',sk_imread(ClassLabels_train['File Path'].iloc[ind]),'Sample from ClassLabels_train')\n",
    "del(ind) # Clear RAM\n",
    "\n",
    "# Index 'i' of ImgArray_test should have exactly same image as Index 'i' of df_test\n",
    "ind = random.randint(0, len(ClassLabels_test))\n",
    "images_on_side(ImgArray_test[ind],'Sample from ImgArray_test',sk_imread(ClassLabels_test['File Path'].iloc[ind]),'Sample from ClassLabels_test')\n",
    "del(ind) # Clear RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8757f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    ** Optional Step **\n",
    "Over-Sampling Training Data\n",
    "    - By Default, Over-Sampling Training Data is Disabled\n",
    "'''\n",
    "\n",
    "# Set 'oversample' to True if Over-Sampling Training Data is Needed\n",
    "oversample = False\n",
    "\n",
    "# Data Properties BEFORE Over Sampling\n",
    "print('  BEFORE Over Sampling : \\n')\n",
    "print('Number of Training Samples : \\n',ClassLabels_train['Class Label'].value_counts().to_string()[12:])\n",
    "print('Shape of Training ImageArray : ',ImgArray_train.shape)\n",
    "print('Number of Training Samples : ',len(ClassLabels_train))\n",
    "\n",
    "# Reset Previous Index to avoid Indexing Issues\n",
    "ClassLabels_train.reset_index(drop=True, inplace=True)\n",
    "ClassLabels_test.reset_index(drop=True, inplace=True)\n",
    "ClassLabels_validation.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Perform Over-Sampling\n",
    "if oversample == True:\n",
    "    max_class_len = max(ClassLabels_train['Class Label'].value_counts())\n",
    "    oversample_df = pd.DataFrame()\n",
    "    oversample_X = np.array(np.zeros((0, IMG_WIDTH, IMG_HEIGHT, IMG_CHANNELS)), dtype='uint8')\n",
    "    for damage in set(ClassLabels_train['Class Label']):\n",
    "        damage_df = ClassLabels_train[ClassLabels_train['Class Label']==damage]\n",
    "        damage_X = ImgArray_train[damage_df.index]\n",
    "        class_len = len(damage_df)\n",
    "        if class_len != max_class_len:\n",
    "            extra = max_class_len - class_len\n",
    "            inds = random.sample(range(class_len), extra)\n",
    "            extra_df = damage_df.iloc[inds]\n",
    "            extra_X = damage_X[inds]\n",
    "            oversample_X = np.vstack([oversample_X, extra_X])\n",
    "            oversample_df = pd.concat([oversample_df, extra_df]).reset_index(drop=True)\n",
    "            del(inds) # Clear RAM\n",
    "            del(damage_X) # Clear RAM\n",
    "            del(damage_df) # Clear RAM\n",
    "            del(extra_df) # Clear RAM\n",
    "            del(extra_df) # Clear RAM\n",
    "    if len(set(ClassLabels_train['Class Label'].value_counts())) != 1:\n",
    "        ClassLabels_train = pd.concat([ClassLabels_train, oversample_df]).reset_index(drop=True)\n",
    "        ImgArray_train = np.vstack([ImgArray_train, oversample_X])\n",
    "        y_train = ClassLabels_train['Class Label']\n",
    "    del(max_class_len) # Clear RAM\n",
    "    del(oversample_df) # Clear RAM\n",
    "    del(oversample_X) # Clear RAM\n",
    "    del(oversample) # Clear RAM\n",
    "\n",
    "# Data Properties AFTER Over-Sampling\n",
    "print('\\n  AFTER Over Sampling : \\n')\n",
    "print('Number of Training Samples : \\n',ClassLabels_train['Class Label'].value_counts().to_string()[12:])\n",
    "print('Shape of Training ImageArray : ',ImgArray_train.shape)\n",
    "print('Number of Training Samples : ',len(ClassLabels_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e70c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    ** Verification Only **\n",
    "Verify that Random Sampling is Successful.\n",
    "  - Index 'i' of ImgArray_train should have exactly same image as Index 'i' of ClassLabels_train\n",
    "  - Verification is done Twice\n",
    "  - NOTE : Ideally, Images on Left and Right should be Exactly Same\n",
    "\n",
    "'''\n",
    "\n",
    "for _ in range(2):\n",
    "    ind = random.randint(0, len(ClassLabels_train))\n",
    "    images_on_side(ImgArray_train[ind],'Sample from ImgArray_train',sk_imread(ClassLabels_train['File Path'].iloc[ind]),'Sample from ClassLabels_train')\n",
    "    del(ind) # Clear RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d402016",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "One-Hot-Encoding for \n",
    "    - Training Class Label Labels\n",
    "    - Testing Class Label Labels\n",
    "    - Validation Class Label Labels\n",
    "'''\n",
    "\n",
    "# Defining One-Hot-Encoding\n",
    "OHE = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# One-Hot-Encoding of Training Class Label Labels\n",
    "ClassLabels_train = np.array(ClassLabels_train['Class Label']).reshape(-1,1)\n",
    "ClassLabels_train = OHE.fit_transform(ClassLabels_train)\n",
    "\n",
    "# One-Hot-Encoding of Testing Class Label Labels\n",
    "ClassLabels_test = np.array(ClassLabels_test['Class Label']).reshape(-1,1)\n",
    "ClassLabels_test = OHE.fit_transform(ClassLabels_test)\n",
    "\n",
    "# One-Hot-Encoding of Validation Class Label Labels\n",
    "ClassLabels_validation = np.array(ClassLabels_validation['Class Label']).reshape(-1,1)\n",
    "ClassLabels_validation = OHE.fit_transform(ClassLabels_validation)\n",
    "\n",
    "OHE_classes = OHE.categories_[0]\n",
    "print('Class Labeles for One-Hot-Encoding Are : \\n',OHE_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babecf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Exporting the following One-Hot-Encoded Data as Pickle Files\n",
    "    - ClassLabels_train\n",
    "    - ClassLabels_test\n",
    "    - ClassLabels_validation\n",
    "Exporting the following as Pickle Files\n",
    "    - ImgArray_train\n",
    "    - ImgArray_test\n",
    "    - ImgArray_validation\n",
    "Exporting One-Hot-Encoding\n",
    "'''\n",
    "\n",
    "# Exporting ClassLabels_train\n",
    "try:\n",
    "    with open (PICKLE_DIR+'ClassLabels_train.pkl', 'wb') as f: pickle.dump(ClassLabels_train, f)\n",
    "    print('Export Successful for : ClassLabels_train.pkl ')  \n",
    "    del(ClassLabels_train) # Clear RAM\n",
    "except:\n",
    "    print('Export Unsuccessful for : ClassLabels_train.pkl ')\n",
    "\n",
    "# Exporting ClassLabels_test\n",
    "try:\n",
    "    with open (PICKLE_DIR+'ClassLabels_test.pkl', 'wb') as f: pickle.dump(ClassLabels_test, f)\n",
    "    print('Export Successful for : ClassLabels_test.pkl ')  \n",
    "    del(ClassLabels_test) # Clear RAM\n",
    "except:\n",
    "    print('Export Unsuccessful for : ClassLabels_test.pkl ')\n",
    "\n",
    "# Exporting ClassLabels_validation\n",
    "try:\n",
    "    with open (PICKLE_DIR+'ClassLabels_validation.pkl', 'wb') as f: pickle.dump(ClassLabels_validation, f)\n",
    "    print('Export Successful for : ClassLabels_validation.pkl ')  \n",
    "    del(ClassLabels_validation) # Clear RAM\n",
    "except:\n",
    "    print('Export Unsuccessful for : ClassLabels_validation.pkl ')\n",
    "\n",
    "    \n",
    "# Exporting ImgArray_train\n",
    "try:\n",
    "    with open (PICKLE_DIR+'ImgArray_train.pkl', 'wb') as f: pickle.dump(ImgArray_train, f)\n",
    "    print('Export Successful for : ImgArray_train.pkl')  \n",
    "    del(ImgArray_train) # Clear RAM\n",
    "except:\n",
    "    print('Export Unsuccessful for : ImgArray_train.pkl ')\n",
    "\n",
    "# Exporting ImgArray_test\n",
    "try:\n",
    "    with open (PICKLE_DIR+'ImgArray_test.pkl', 'wb') as f: pickle.dump(ImgArray_test, f)\n",
    "    print('Export Successful for : ImgArray_test.pkl')\n",
    "    del(ImgArray_test) # Clear RAM\n",
    "except:\n",
    "    print('Export Unsuccessful for : ImgArray_test.pkl ') \n",
    "\n",
    "# Exporting ImgArray_validation\n",
    "try:\n",
    "    with open (PICKLE_DIR+'ImgArray_validation.pkl', 'wb') as f: pickle.dump(ImgArray_validation, f)\n",
    "    print('Export Successful for : ImgArray_validation.pkl')\n",
    "    del(ImgArray_validation) # Clear RAM\n",
    "except:\n",
    "    print('Export Unsuccessful for : ImgArray_validation.pkl ') \n",
    "\n",
    "    \n",
    "# Exporting One-Hot-Encoding \n",
    "try:\n",
    "    with open (PICKLE_DIR+'OHE.pkl', 'wb') as f: pickle.dump(OHE, f)\n",
    "    print('Export Successful for : OHE.pkl')\n",
    "    del(OHE) # Clear RAM\n",
    "except:\n",
    "    print('Export Unsuccessful for : OHE.pkl ') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fab41a4-8ee1-4d28-aa5e-55d6596c0c4a",
   "metadata": {},
   "source": [
    "More At : https://github.com/iSiddharth20/DeepLearning-ImageClassification-Toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bc0e74-0129-4052-b8c1-5649b0b500bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

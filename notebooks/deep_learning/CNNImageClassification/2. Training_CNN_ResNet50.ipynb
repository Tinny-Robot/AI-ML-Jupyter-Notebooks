{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd400876-f589-4f9e-8a48-326b58f26389",
   "metadata": {},
   "source": [
    "### Understanding Model Creating and Training of CNN ResNet50\n",
    "- Training, Testing, Validation Data and One-Hot-Encoding are Imported\n",
    "    - All Data is converted to TensorFlow Format\n",
    "- Learning Rate Scheduler is Defined (Change If Desired) \n",
    "- Stochastic Gradient Descent with Momentum is Used as Optimizer  (Change If Desired) \n",
    "- Added Data Augmentation Techniques to improve Model Learning  (Change If Desired) \n",
    "- Base Model (CNN ResNet50) is Loded from TensorFlow Library\n",
    "    - Custom Optimal Changes have been made to the Structure\n",
    "    - Final Model is Compiled\n",
    "- Final Model is Trained\n",
    "    - Final Model with Lowest Validation Loss is Exported as a '.h5' file\n",
    "- Traning Time (In Seconds) is Displayed\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4638d9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Working Directories\n",
    "Name of Model\n",
    "'''\n",
    "import os\n",
    "\n",
    "# Directory where Pickle Files will be Stored\n",
    "PICKLE_DIR = '../PickleFiles/'\n",
    "\n",
    "# Directory where Models will be Stored (Folder will be Creted By Code)\n",
    "MODEL_DIR = '../TrainedModels/'\n",
    "os.makedirs(os.path.dirname(MODEL_DIR), exist_ok=True)\n",
    "\n",
    "# Name of Model Used\n",
    "MODEL_NAME = 'CNN_ResNet50'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1f5a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Importing Necessary Libraries and Packages\n",
    "'''\n",
    "# Disable TensorFlow Warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Helpers\n",
    "import pickle\n",
    "\n",
    "# To log Trainnig Time\n",
    "from datetime import datetime\n",
    "\n",
    "# Importing TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756ddf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Importing the following One-Hot-Encoded Data from Pickle Files\n",
    "    - ClassLabels_train\n",
    "    - ClassLabels_test\n",
    "    - ClassLabels_validation\n",
    "Importing the following from Pickle Files\n",
    "    - ImgArray_train\n",
    "    - ImgArray_test\n",
    "    - ImgArray_validation\n",
    "Importing One-Hot-Encoding\n",
    "'''\n",
    "\n",
    "# Exporting ClassLabels_train\n",
    "try:\n",
    "    with open(PICKLE_DIR+'ClassLabels_train.pkl', 'rb') as f: ClassLabels_train = pickle.load(f)\n",
    "    print('Import Successful for : ClassLabels_train.pkl ')  \n",
    "except:\n",
    "    print('Import Unsuccessful for : ClassLabels_train.pkl ')\n",
    "\n",
    "# Exporting ClassLabels_test\n",
    "try:\n",
    "    with open(PICKLE_DIR+'ClassLabels_test.pkl', 'rb') as f: ClassLabels_test = pickle.load(f)\n",
    "    print('Import Successful for : ClassLabels_test.pkl ')  \n",
    "except:\n",
    "    print('Import Unsuccessful for : ClassLabels_test.pkl ')\n",
    "\n",
    "# Exporting ClassLabels_validation\n",
    "try:\n",
    "    with open(PICKLE_DIR+'ClassLabels_validation.pkl', 'rb') as f: ClassLabels_validation = pickle.load(f)\n",
    "    print('Import Successful for : ClassLabels_validation.pkl ')  \n",
    "except:\n",
    "    print('Import Unsuccessful for : ClassLabels_validation.pkl ')\n",
    "\n",
    "    \n",
    "# Exporting ImgArray_train\n",
    "try:\n",
    "    with open(PICKLE_DIR+'ImgArray_train.pkl', 'rb') as f: ImgArray_train = pickle.load(f)\n",
    "    print('Import Successful for : ImgArray_train.pkl')  \n",
    "except:\n",
    "    print('Import Unsuccessful for : ImgArray_train.pkl ')\n",
    "\n",
    "# Exporting ImgArray_test\n",
    "try:\n",
    "    with open(PICKLE_DIR+'ImgArray_test.pkl', 'rb') as f: ImgArray_test = pickle.load(f)\n",
    "    print('Import Successful for : ImgArray_test.pkl')\n",
    "except:\n",
    "    print('Import Unsuccessful for : ImgArray_test.pkl ') \n",
    "\n",
    "# Exporting ImgArray_validation\n",
    "try:\n",
    "    with open(PICKLE_DIR+'ImgArray_validation.pkl', 'rb') as f: ImgArray_validation = pickle.load(f)\n",
    "    print('Import Successful for : ImgArray_validation.pkl')\n",
    "except:\n",
    "    print('Import Unsuccessful for : ImgArray_validation.pkl ') \n",
    "\n",
    "    \n",
    "# Exporting One-Hot-Encoding \n",
    "try:\n",
    "    with open(PICKLE_DIR+'OHE.pkl', 'rb') as f: OHE = pickle.load(f)\n",
    "    print('Import Successful for : OHE.pkl')\n",
    "except:\n",
    "    print('Import Unsuccessful for : OHE.pkl ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea3aaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Preparing Data Pipeline\n",
    "    - Convert All Data to TensorFlow Format\n",
    "    - Defining Data Augmentation Function\n",
    "    - Defining Rescaled Image Height, Width\n",
    "\n",
    "Change BATCH_SIZE as per Requirement.\n",
    "    - Higher the BATCH_SIZE (usually 16-32) the better\n",
    "    - If you get 'Resource Exhaust Error', decrease the BATCH_SIZE\n",
    "'''\n",
    "\n",
    "BATCH_SIZE = 8 \n",
    "IMG_HEIGHT = 400\n",
    "IMG_WIDTH = 600\n",
    "IMG_CHANNELS = 3 # Assuming RGB Image\n",
    "\n",
    "# Training Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((ImgArray_train,ClassLabels_train))\n",
    "train_dataset = train_dataset.shuffle(10000, reshuffle_each_iteration=True)\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.repeat()\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "del(ImgArray_train) # Clear RAM\n",
    "\n",
    "# Testing Dataset\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((ImgArray_test,ClassLabels_test))\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "del(ImgArray_test) # Clear RAM\n",
    "del(ClassLabels_test) # Clear RAM\n",
    "\n",
    "# Validation Dataset\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((ImgArray_validation,ClassLabels_validation))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "del(ImgArray_validation) # Clear RAM\n",
    "\n",
    "# Data Augmentation\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "  tf.keras.layers.experimental.preprocessing.RandomTranslation(0.03, 0.06),\n",
    "  tf.keras.layers.experimental.preprocessing.RandomRotation(0.015),\n",
    "  tf.keras.layers.experimental.preprocessing.RandomHeight(0.1, interpolation='nearest'),\n",
    "  tf.keras.layers.experimental.preprocessing.RandomWidth(0.1), \n",
    "])\n",
    "\n",
    "# Steps Per Epoch and Validation Steps\n",
    "steps_per_epoch = len(ClassLabels_train) // BATCH_SIZE\n",
    "validation_steps = len(ClassLabels_validation) // BATCH_SIZE\n",
    "del(ClassLabels_train) # Clear RAM\n",
    "del(ClassLabels_validation) # Clear RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c577ee-90de-45a9-8579-abc95d2c71ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Callbacks\n",
    "earlystopper = tf.keras.callbacks.EarlyStopping(monitor='val_auc', patience=10, verbose=2)\n",
    "checkpointer = tf.keras.callbacks.ModelCheckpoint(MODEL_DIR+MODEL_NAME+'.h5', save_best_only=True ,verbose=1)\n",
    "\n",
    "# Learning Rate Scheduler (Change as per Requirement)\n",
    "initial_learning_rate = 0.001\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, mode='min', cooldown=1, min_lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc23e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Building Training Model [ResNet50]\n",
    "    - Load Base Model\n",
    "    - Build Final Model\n",
    "    - Compile Final Model\n",
    "'''\n",
    "\n",
    "# Loading Base Model\n",
    "OHE_classes = OHE.categories_[0]\n",
    "num_classes = len(OHE_classes)\n",
    "del(OHE) # Clear RAM\n",
    "base_model = tf.keras.applications.resnet50.ResNet50(\n",
    "    input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), include_top=False, \n",
    "    weights='imagenet', classes=num_classes\n",
    ")\n",
    "\n",
    "# Building Final Model\n",
    "inputs = tf.keras.layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "x = data_augmentation(inputs)\n",
    "x = base_model(x)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compiling Final Model\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=initial_learning_rate, momentum=0.9),\n",
    "    metrics=['accuracy', Precision(name='precision'), Recall(name='recall'), AUC(name='auc')])\n",
    "\n",
    "# View the Model Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269629f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Train the Model\n",
    "'''\n",
    "begin = datetime.now()\n",
    "\n",
    "EPOCHS = 50 # Change as per Requirement\n",
    "history = model.fit(train_dataset, validation_data=val_dataset, \n",
    "                    steps_per_epoch=steps_per_epoch, validation_steps=validation_steps, \n",
    "                    epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1, \n",
    "                    callbacks = [checkpointer, earlystopper, lr_scheduler])\n",
    "\n",
    "finish = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa3481f-3e8c-4768-9659-c058afea54ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Display Time Required (In Seconds) to Train the Model\n",
    "'''\n",
    "\n",
    "total_time = finish-begin\n",
    "print('Total Training Time (In Seconds) for '+MODEL_NAME+' : ',total_time.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb8ddf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Evaluate the Model\n",
    "'''\n",
    "\n",
    "model = tf.keras.models.load_model(MODEL_DIR+MODEL_NAME+'.h5')\n",
    "\n",
    "print('Training : ', model.evaluate(train_dataset, steps=steps_per_epoch))\n",
    "print('Validation : ', model.evaluate(val_dataset))\n",
    "print('Testing : ', model.evaluate(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7082894-40c3-49a9-85df-fe6221c5079c",
   "metadata": {},
   "source": [
    "More At : https://github.com/iSiddharth20/DeepLearning-ImageClassification-Toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eef81e-0ff9-4b6d-b6c0-b955c49b9a15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
